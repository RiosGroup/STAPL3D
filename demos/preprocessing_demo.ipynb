{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAPL-3D preprocessing demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the core components of the STAPL3D preprocessing pipeline: **z-stack shading** correction and **3D inhomogeneity** correction. \n",
    "\n",
    "If you did not follow the STAPL-3D README: please find STAPL-3D and the installation instructions [here](https://github.com/RiosGroup/STAPL3D) before doing this demo.\n",
    "\n",
    "Because STAPL-3D is all about big datafiles, we provide small cutouts and precomputed summary data that will be downloaded while progressing through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define where you have want the data to be downloaded by changing *projectdir*; default is the current demo directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import urllib.request\n",
    "\n",
    "projectdir = '.'\n",
    "dataset = 'HFK16w'\n",
    "\n",
    "datadir = os.path.join(projectdir, dataset)\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "filestem = os.path.join(datadir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define STAPL3D parameters preferably using a [yaml](https://yaml.org) parameter file. It has a simple structure and can be parsed in Python and `bash`. We will download the example, read it into a dictionary structure, list all entries and show the entry that contains information on the default directory structure for STAPL3D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dirtree', 'dataset', 'shading', 'stitching', 'splitchannels', 'mask', 'biasfield', 'bias_stack', 'biasfield_apply', 'ims_aggregate', 'splitblocks', 'membrane_enhancement', 'segmentation', 'segmentation_postproc', 'segmentation_gather', 'relabel', 'relabel_gather', 'copyblocks', 'copyblocks_gather', 'zipping', 'zipping_gather', 'ziplines', 'zipquads', 'zipping_postproc', 'copydataset', 'subsegment', 'mergeblocks', 'features', 'features_postproc'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'datadir': {'blocks': 'blocks',\n",
       "  'channels': 'channels',\n",
       "  'shading': 'shading',\n",
       "  'mask': None,\n",
       "  'biasfield': 'biasfield',\n",
       "  'profiling': 'profiling',\n",
       "  'jobfiles': 'jobfiles'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_file = '{}.yml'.format(filestem)\n",
    "\n",
    "# Download the yml-file.\n",
    "if not os.path.exists(parameter_file):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/Ubx9wVon5CIuIzo/download'\n",
    "    urllib.request.urlretrieve(url, parameter_file)\n",
    "\n",
    "# Load parameter file.\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "# Show contents.\n",
    "cfg.keys()\n",
    "cfg['dirtree']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shading correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shading correction (or flatfield correction) attempts to remove the intensity gradients that may be present in the xy-plane of the z-stacks that make up the dataset. These originate from imperfections in the microscope's optics and manifest as a grid over the assembled 3D volume. Because the shading is channel-specific, STAPL-3D estimates a 2D profile for each channel separately from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a 2-ch z-stack of data (106 x 1024 x 1024 x 2) in the data archive for demonstration purposes. These are two channels extracted from an 8-channel dataset of 262 stacks, i.e. ~0.1% of the data. The stack includes a nuclear channel (DAPI) and a membrane channel (NCAM1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the czi-file.\n",
    "\n",
    "czi_filepath = '{}.czi'.format(filestem)\n",
    "if not os.path.exists(czi_filepath):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/Ly85srzZmdWJCyJ/download'\n",
    "    urllib.request.urlretrieve(url, czi_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the parameters to the shading correction module in the yaml parameter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_format': 'czi',\n",
       " 'params': {'postfix': '_shading',\n",
       "  'metric': 'median',\n",
       "  'noise_threshold': 1000,\n",
       "  'z_range': None,\n",
       "  'quantile_threshold': 0.9,\n",
       "  'polynomial_order': 3},\n",
       " 'submit': {'n_workers': None,\n",
       "  'channels': None,\n",
       "  'array': 'channel',\n",
       "  'nodes': 1,\n",
       "  'tasks': 1,\n",
       "  'mem': '20G',\n",
       "  'wtime': '02:00:00'},\n",
       " 'conda': {'env': 'stapl3d'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load parameter file.\n",
    "cfg['shading']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that, in this example, we calculate the *median* value for z-stacks concatenated over X and Y, while masking any value < *1000*. We use the *90%* of planes that have the highest median intensities to calculate the 1D shading profile that is fit using a *3rd order* polynomial. The resulting files of this processing step are postfixed with *_shading* \n",
    "\n",
    "The estimation of the shading profile is done in parallel for channels. The number of concurrent processes can be set by specifying 'n_workers' in the yml-file, or as an argument. The default is to use the same number of processors as there are channels in the dataset--if available. \n",
    "\n",
    "Note that for cluster-deployment (SGE or SLURM), more specific configurations can be set in the yaml (cfg['shading_estimation']).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the shading estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'stapl3d.preprocessing.shading' has no attribute 'estimate_channel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b1de05d5a8bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mczi_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}.czi'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilestem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mshading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_channel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mczi_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'shading'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'stapl3d.preprocessing.shading' has no attribute 'estimate_channel'"
     ]
    }
   ],
   "source": [
    "# non-parallelized\n",
    "from stapl3d.preprocessing import shading\n",
    "czi_filepath = '{}.czi'.format(filestem)\n",
    "channel = 0\n",
    "shading.estimate_channel(czi_filepath, channel, **cfg['shading']['params'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "child index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4473c01d980b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstapl3d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mczi_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}.czi'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilestem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mshading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mczi_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/stapl3d/stapl3d/preprocessing/shading.py\u001b[0m in \u001b[0;36mestimate\u001b[0;34m(image_in, parameter_file, step_id, outputdir, n_workers, channels, planes, metric, noise_threshold, z_range, quantile_threshold, polynomial_order, postfix)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0msubparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'submit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0miminfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_image_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m     \u001b[0msubparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'channels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0msubparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'planes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'planes'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'planes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/stapl3d/stapl3d/preprocessing/shading.py\u001b[0m in \u001b[0;36mget_image_info\u001b[0;34m(image_in)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0mzyxc_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dims_zyxc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'zstack_shape'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzyxc_idxs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elsize_zyxc'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mczi_get_elsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mczi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0miminfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stack_offsets'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mczi_tile_offsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mczi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miminfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/stapl3d/stapl3d/preprocessing/shading.py\u001b[0m in \u001b[0;36mczi_get_elsize\u001b[0;34m(czi)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# FIXME: use find() method on etree items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0melsize_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m     \u001b[0melsize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0melsize_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: child index out of range"
     ]
    }
   ],
   "source": [
    "# parallelized\n",
    "from stapl3d.preprocessing import shading\n",
    "czi_filepath = '{}.czi'.format(filestem)\n",
    "shading.estimate(czi_filepath, parameter_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each channel, this will write the estimated shading profile as an image (.tif) and a processing report (.pdf), as well as the calculated medians (.pickle) and a logfile (.log) to the *HFK16w/shading/* directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(datadir, 'shading'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the single stack these do not look great, because the algorithm needs multiple z-stacks to reliably estimate the shading profile. Therefore, we provide pre-calculated medians for the full 262-stack dataset (in *HFK16w/shading_full*) to demonstrate the expected output. First, let's plot the results for a single channel: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract the shading_full directory.\n",
    "\n",
    "shading_dir = os.path.join(datadir, 'shading_full')\n",
    "zipfilepath = '{}.zip'.format(shading_dir)\n",
    "\n",
    "if not os.path.exists(shading_dir):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/izqKOK7kQzr1nG1/download'\n",
    "    urllib.request.urlretrieve(url, zipfilepath)\n",
    "\n",
    "    import zipfile\n",
    "    with zipfile.ZipFile(zipfilepath, 'r') as zf:\n",
    "        zf.extractall(datadir)\n",
    "\n",
    "# Generate and show the report of channel 0\n",
    "shading.generate_report(shading_dir, 'HFK16w_full', channel=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row shows the result for concatenating the data over *X*, i.e. yielding a median value for each *yz*-coordinate. The left plot shows the medians of the planes (selected using a *quantile_threshold* parameter of 0.8) in rainbow colours. The right plot shows the normalized profile with confidence intervals as well as the normalized fit. The bottom left shows the median profile over *z*, with the selected planes indicated by tick marks. The bottom right image shows the 2D shading profile to use for correcting each plane in each z-stack of the channel. The red dashed traces indicates an -arbitrary- threshold to help with flagging potential issues with the data; the viridis colormap is also clipped to red  at this threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run it for all channnels, sending the output to pdf's in *HFK16w/shading_full*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ch in range(8):\n",
    "    shading.generate_report(shading_dir, 'HFK16w_full', channel=ch, ioff=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use your own imaging data, we stitch in the proprietary Zeiss Zen or Arivis software packages. Please stitch and then convert the result to an Imaris or STAPL-3D file-format. For the bias field estimation demo below, we provide a downsampled image of the stitching result in hdf5 format. For the segmentation demo, we provide a cutout of the fully preprocessed file in Imaris format: *HFK16w_shading_stitching_biasfield.ims*. A free viewer for these data can be downloaded [here](https://imaris.oxinst.com/imaris-viewer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split channels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inhomogeneity correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next correct the stitched file for inhomogeneities such as depth attenuation and uneven penetration of clearing agents and antibodies. This is done using the *N4* algorithm ([Tustison et al., 2010](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3071855) as implemented in simpleitk) on a downsampled image. For this demo, we provide the downsampled data in an hdf5 file.\n",
    "\n",
    "We download the data, use the STAPL-3D Image class to get some info about this image, and display the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stitch_stem = '{}{}{}'.format(\n",
    "    dataset,\n",
    "    cfg['shading']['postfix'],\n",
    "    cfg['stitching']['postfix'],\n",
    ")\n",
    "bfc_filepath = os.path.join(datadir, '{}.h5'.format(stitch_stem))\n",
    "\n",
    "# Download the hdf5-file.\n",
    "if not os.path.exists(bfc_filepath):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/WkMMCW5e4wgNUgb/download'\n",
    "    urllib.request.urlretrieve(url, bfc_filepath)\n",
    "\n",
    "# Print image info.\n",
    "from stapl3d import Image\n",
    "image_in = '{}/data'.format(bfc_filepath)\n",
    "im = Image(image_in)\n",
    "im.load(load_data=False)\n",
    "im.get_props()\n",
    "im.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image dimensions are *zyxc* = *106 x 263 x 249 x 8* with voxels of *1.2 x 21.3 x 21.3* $\\mu$m. This is a good size for estimating the slow variations over the volume. Default parameters are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['biasfield']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an imaris pyramid image is provided, data will be taken at `resolution_level` and further downsampled with `downsample_factors`. The `n_iterations`, `n_fitlevels` and `n_bspline_cps` are passed to the [ITK-filter](https://simpleitk.org/doxygen/latest/html/classitk_1_1simple_1_1N4BiasFieldCorrectionImageFilter.html). \n",
    "\n",
    "To use a mask in the estimation, the `mask_in` input can either be \n",
    " - set to `True`, in which case the path defaults to `{dataset}{cfg['mask']['postfix']}.h5/mask`\n",
    " - contain the path to a mask image (in which background should be `0`).\n",
    " The mask image is expected to be the same size as the input image, i.e. it will also be downsampled with `downsample_factors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the N4 algorithm (on channel 4). SimpleITK will keep **all your processors busy for a while** in processing a single channel. (Note that for HPC cluster deployment, there is more control: channels are distributed over separate jobs, and the number of threads used for each channel can be set separately.)\n",
    "\n",
    "Because the hdf5 file already contains downsampled data, we set `downsample_factors` to unitary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.preprocessing import biasfield\n",
    "\n",
    "channel = 4\n",
    "biasfield_dir = os.path.join(datadir, cfg['dirtree']['datadir']['biasfield'])\n",
    "\n",
    "biasfield.estimate_channel(\n",
    "    image_in,\n",
    "    channel=channel,\n",
    "    downsample_factors=[1, 1, 1, 1, 1],\n",
    "    n_iterations=2,\n",
    "    n_fitlevels=4,\n",
    "    n_bspline_cps=[5, 5, 5],\n",
    "    outputdir=biasfield_dir,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we (re)generate the bias field correction report to inspect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.preprocessing import biasfield\n",
    "biasfield.generate_report(biasfield_dir, stitch_stem, channel=channel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left column shows orthogonal sections of the downsampled dataset for the uncorrected (top) and corrected data (middle) as well as the estimated bias field (bottom). Plotted on the left and top of the images are profiles of the median values over the three axes. The right column offers a closer comparison of the profiles (*mean + SD*) for the corrected (green) vs uncorrected (red) data. The bias field correction yields a much flatter profile for *z*, as well as *xy*. Low-frequency inhomogeneities are removed, while the detail of the specific staining is retained in the corrected data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, we called the function for estimation for a single channel directly, foregoing the yaml file and specifying the parameters directly as arguments. To estimate the inhomogeneity for all channels a single call to the `biasfield.estimate` function suffices, after which `biasfield.apply` can be called to correct the high-resolution volumes:\n",
    "\n",
    "```\n",
    "biasfield.estimate(ims_file, par_file)\n",
    "biasfield.apply(ims_file, par_file)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stapl3d",
   "language": "python",
   "name": "stapl3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
