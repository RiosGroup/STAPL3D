{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAPL-3D segmentation demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the core components of the STAPL-3D segmentation pipeline: **blockwise segmentation** and **zipping**.\n",
    "\n",
    "If you did not follow the STAPL-3D README: please find STAPL-3D and the installation instructions [here](https://github.com/RiosGroup/STAPL3D) before doing this demo.\n",
    "\n",
    "Because STAPL-3D is all about big datafiles, we provide small cutouts and precomputed summary data. Please, download [HFK16w.zip](https://surfdrive.surf.nl/files/index.php/s/Q9wRT5cyKGERxI5) (~6GB). Note that the Preprocessing and Segmentation demo's use the same zip-file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define where you have put the data. Please change *datadir* to point to the *HFK16w* directory that you have unzipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "datadir = './HFK16w'\n",
    "dataset = 'HFK16w'\n",
    "filestem = os.path.join(datadir, dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided a data cutout in the Imaris v5.5 file format, which is an hdf5 file with 5 dimensions. In processing the full dataset, this would equate to a single datablock; for this demo we will further subdivide this blocks to demonstrate the pipeline.\n",
    "\n",
    "We use the STAPL-3D Image class to load this file and inspect it's properties. We'll also save the dimensions, the Z-dimension and the number of channels in convenience variables `dims`, `Z` and `C`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import Image\n",
    "\n",
    "image_in = '{}_bfc_block.ims'.format(filestem)\n",
    "\n",
    "im = Image(image_in)\n",
    "im.load(load_data=False)\n",
    "\n",
    "props = im.get_props()\n",
    "\n",
    "im.close()\n",
    "\n",
    "dims = im.dims\n",
    "Z = im.dims[im.axlab.index('z')]\n",
    "C = im.dims[im.axlab.index('c')]\n",
    "\n",
    "props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For segmentation, we use a weighted sum of the membrane channels (ch3, ch5, ch6, ch7). The weights [0.5, 0.5, 1.0, 1.0] work well for this data.\n",
    "We have specified this in the parameter file HFK16w.yml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "parameter_file = '{}.yml'.format(filestem)\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "cfg['channels']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above indicates that, in addition to the membrane sum, we generate a nuclear channel mean as well as a mean over all channels (used for generating masks). Importantly, we specify that we want to output channel 0 (DAPI), because we will use it to create a nuclear mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the shape of the processing blocks. Usually we would opt for a blocksize of ~100-200 million voxels; now we chose a blocksize in *xy* of 176 for 64 blocks of ~6M voxels. We keep the margin similar to what we set for big datasets as reducing it may hinder adequate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 176  # blocksize\n",
    "bm = 64  # blockmargin\n",
    "\n",
    "blocksize = [Z, bs, bs, C, 1]\n",
    "blockmargin = [0, bm, bm, 0, 0]\n",
    "\n",
    "blockdir = os.path.join(datadir, 'blocks_{:04d}'.format(bs))\n",
    "block_prefix = os.path.join(blockdir, '{}_bfc_block'.format(dataset))\n",
    "os.makedirs(blockdir, exist_ok=True)\n",
    "\n",
    "'Processing data in blocks of {} voxels with a margin of {} voxels'.format(blocksize, blockmargin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the function that computes the membrane mean, and splits the data into blocks at the same time. Datablocks are written to the *HFK16w/blocks/* directory and are postfixed by the voxel coordinates of the original datafile HFK16w/blocks/HFK16w_**x-X_y_Y_z-Z**.h5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.channels import process_channels\n",
    "\n",
    "process_channels(\n",
    "    image_in,\n",
    "    parameter_file,\n",
    "    blocksize,\n",
    "    blockmargin,\n",
    "    outputprefix=block_prefix,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the files that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "ipf = ''\n",
    "filelist = glob(os.path.join(blockdir, '{}_*{}.h5'.format(dataset, ipf)))\n",
    "filelist.sort()\n",
    "len(filelist), filelist[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the groups and dataset (internal h5 file structure) with h5py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(filelist[20],'r') as f:\n",
    "    f.visit(print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting hdf5 files have the following internal file structure:\n",
    "    - .h5/mean\n",
    "    - .h5/chan/ch00\n",
    "    - .h5/memb/mean\n",
    "    - .h5/nucl/mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membrane enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before segmentation, we perform membrane enhancement. For the demo we do not want to be dependent on the third-party ACME software and provide the output that otherwise results from the ACME procedure. We split it into blocks, and write it as separate datasets in the same files as the channel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.channels import splitblocks\n",
    "\n",
    "for ids in ['memb/preprocess', 'memb/planarity']:\n",
    "    image_in = '{}_bfc_block_ACME.h5/{}'.format(filestem, ids)\n",
    "    output_template = '{}_{}.h5/{}'.format(block_prefix, '{}', ids)\n",
    "    splitblocks(image_in, [106, bs, bs], [0, bm, bm], output_template)\n",
    "\n",
    "\n",
    "# from stapl3d.channels import h5_nii_convert\n",
    "# filestem = 'HFK16w_bfc_block_00336-00664_00936-01264_00000-00106'\n",
    "# image_in = os.path.join(blockdir, '{}.h5/memb/mean'.format(filestem))\n",
    "# image_out = os.path.join(blockdir, '{}_memb-mean.nii.gz'.format(filestem))\n",
    "# h5_nii_convert(image_in, image_out)\n",
    "\n",
    "# ... ACME ...\n",
    "\n",
    "# for vol in ['preprocess', 'planarity']:\n",
    "#     image_in = os.path.join(blockdir, '{}_memb-{}.nii.gz'.format(filestem, vol))\n",
    "#     image_out = os.path.join(blockdir, '{}_foo.h5/memb/{}'.format(filestem, vol))\n",
    "#     h5_nii_convert(image_in, image_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation is parallelized over the blocks we just created. Each of the 64 files is processed seperately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "ipf = ''\n",
    "filepat = '{}_*{}.h5'.format(dataset, ipf)\n",
    "filelist = glob(os.path.join(blockdir, filepat))\n",
    "filelist.sort()\n",
    "\n",
    "len(filelist), filelist[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation routine is associated with a fair amount of parameters. This list all the parameters specified in the yml-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "parameter_file = '{}.yml'.format(filestem)\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "cfg['segmentation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few parameter of particular note:\n",
    "- input volumes:\n",
    "        'ids_memb_mask': 'memb/planarity'\n",
    "        'ids_memb_chan': 'memb/mean'\n",
    "        'ids_nucl_chan': 'chan/ch00'\n",
    "        'ids_dset_mean': 'mean'\n",
    "\n",
    "The following parameters can be changed to optimize segmentation or use parameters from automated fine tuning:\n",
    "- membrane mask:\n",
    "    - 'planarity_thr': 0.0005\n",
    "- nuclei mask:\n",
    "    - 'sauvola_window_size': [19, 75, 75]\n",
    "    - 'dapi_thr': 5000\n",
    "    - 'dapi_absmin': 1000\n",
    "- peak detection:\n",
    "    - 'peaks_size': [11, 19, 19]\n",
    "    - 'compactness': 0.8\n",
    "- watershed:\n",
    "    - 'memb_sigma': 3.0\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the segments for each block. The 106 x 240 x 240 blocksize (including the margin) will take ~1GB of memory per process. Please set the number of processes so that you will stay within RAM. n_proc = 8 would be a fairly safe bet for modern systems. Segmentation time of single block is in the minutes-range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Python's multiprocessing for parallel processing if calling the function from within a Python interpreter. A list of argument-tuples is generated to serve as input, so let's look at the arguments of the segmentation function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from stapl3d.segmentation.segment import extract_segments\n",
    "extract_segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We specify 6 arguments per job, where specific arguments point to the datasets in a particular .h5 datablock-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arglist = []\n",
    "for datafile in filelist:\n",
    "    args = [\n",
    "        '{}/{}/{}'.format(datafile, 'memb', 'planarity'),\n",
    "        '{}/{}/{}'.format(datafile, 'memb', 'mean'),\n",
    "        '{}/{}/{}'.format(datafile, 'chan', 'ch00'),\n",
    "        '{}/{}'.format(datafile, 'mean'),\n",
    "        parameter_file,\n",
    "        datafile.replace('.h5', ''),\n",
    "        True,\n",
    "        ]\n",
    "    arglist.append(tuple(args))\n",
    "\n",
    "with multiprocessing.Pool(processes=n_proc) as pool:\n",
    "    pool.starmap(extract_segments, arglist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report pages (pdf) have been written to the *HFK16w/blocks/* directory. Let's look at one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation.segment import generate_report\n",
    "\n",
    "image_in = '{}/memb/mean'.format(filelist[20])\n",
    "generate_report(image_in, ioff=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From left to right, images are show for:\n",
    " - the DAPI channel and the membrane mean\n",
    " - the nuclear mask and the membrane mask\n",
    " - the combined mask with detected peaks and overlaid on the distance transform image\n",
    " - the first and the final watershed results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having parallelized the segmentation process for increased analysis speed and reduced memory footprint, the need arises to reassemble the blocks into a final combined segmentation volume without seams at the block boundaries. These seams are a consequence of trivial parallelization in processing the individual blocks (i.e. without communication between the processes). They manifest through partial cells lying on the block boundaries that have been assigned different labels in different blocks. Importantly, these doubly segmented cells may not perfectly match up over the boundary. These block-boundary-segments need to be resegmented in order to complete the accurate segmentation of the full dataset. We refer to this correct reassembly of the datablocks as ‘zipping’. In short, it consists of identifying the segments lying on the boundaries, removing them, and resegmenting that space. We aimed to design the procedure such that it requires minimal computational resources and expertise (fast, with a low memory footprint, and without the need for communication between processes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relabel\n",
    "We first perform a sequential relabeling of all the blocks to make each label unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation.zipping import get_maxlabels_from_attribute\n",
    "from stapl3d.segmentation.zipping import relabel_parallel\n",
    "\n",
    "grp = 'segm'\n",
    "ids = 'labels_memb_del'\n",
    "postfix = 'relabeled'\n",
    "\n",
    "# Write the maximum label of each block to a file.\n",
    "filename = '{}_maxlabels_{}.txt'.format(dataset, ids)\n",
    "maxlabelfile = os.path.join(blockdir, filename)\n",
    "maxlabels = get_maxlabels_from_attribute(filelist, '{}/{}'.format(grp, ids), maxlabelfile)\n",
    "\n",
    "# The relabel_parallel function has four arguments: \n",
    "# inputdataset, block index, outputfile and outputpostfix\n",
    "arglist = []\n",
    "for block_idx, datafile in enumerate(filelist):\n",
    "    args = [\n",
    "        '{}/{}/{}'.format(datafile, grp, ids),\n",
    "        block_idx,\n",
    "        maxlabelfile,\n",
    "        postfix,\n",
    "    ]\n",
    "    arglist.append(tuple(args))\n",
    "\n",
    "with multiprocessing.Pool(processes=n_proc) as pool:\n",
    "    pool.starmap(relabel_parallel, arglist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### copy blocks\n",
    "We copy the relabeled blocks to new datasets in the same file for in-place zipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation.zipping import copy_blocks_parallel\n",
    "\n",
    "grp = 'segm'\n",
    "ids = 'labels_memb_del_relabeled'\n",
    "postfix = 'fix'\n",
    "\n",
    "# Write the maximum label of each block to a file.\n",
    "filename = '{}_maxlabels_{}.txt'.format(dataset, ids)\n",
    "maxlabelfile = os.path.join(blockdir, filename)\n",
    "maxlabels = get_maxlabels_from_attribute(filelist, '{}/{}'.format(grp, ids), maxlabelfile)\n",
    "\n",
    "# The copy_blocks_parallel function has three arguments: \n",
    "# inputdataset, block index, outputpostfix\n",
    "arglist = []\n",
    "for block_idx, datafile in enumerate(filelist):\n",
    "    args = [\n",
    "        '{}/{}/{}'.format(datafile, grp, ids),\n",
    "        block_idx,\n",
    "        postfix,\n",
    "    ]\n",
    "    arglist.append(tuple(args))\n",
    "\n",
    "with multiprocessing.Pool(processes=n_proc) as pool:\n",
    "    pool.starmap(copy_blocks_parallel, arglist)\n",
    "\n",
    "\n",
    "# Write a maxlabelfile in which the maxlabels are tracked during zipping.\n",
    "pf = 'relabeled_fix'\n",
    "maxlabelfile = os.path.join(blockdir, '{}_maxlabels_{}.txt'.format(dataset, pf))\n",
    "maxlabels = get_maxlabels_from_attribute(filelist, 'segm/labels_memb_del_{}'.format(pf), maxlabelfile)\n",
    "'maxlabs after copy {}'.format(maxlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip\n",
    "Next, we define the zipping parameters and functions. First, we set the number of processors and the block-layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_proc_max = n_proc\n",
    "\n",
    "# Set the number of seams in the data.\n",
    "n_seams_yx = [7, 7]  # we have 8 x 8 blocks in the HFK16w dataset with bs=176\n",
    "\n",
    "seams = list(range(np.prod(n_seams_yx)))\n",
    "seamgrid = np.reshape(seams, n_seams_yx)\n",
    "seamgrid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the zipping parameters are defined as well as functions to turn the parameters into arguments for the zipping-steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation.zipping import resegment_block_boundaries\n",
    "\n",
    "# Arguments to `resegment_block_boundaries`\n",
    "images_in = ['{}/{}/{}_{}'.format(datafile, 'segm', 'labels_memb_del', pf)\n",
    "             for datafile in filelist]\n",
    "blocksize=[Z, bs, bs]\n",
    "blockmargin=[0, bm, bm]\n",
    "axis=0\n",
    "seamnumbers=[-1, -1, -1]\n",
    "mask_dataset=''\n",
    "relabel=False\n",
    "maxlabel=maxlabelfile\n",
    "in_place=True\n",
    "outputstem=os.path.join(blockdir, dataset)\n",
    "save_steps=False\n",
    "args = [\n",
    "    images_in,\n",
    "    blocksize,\n",
    "    blockmargin,\n",
    "    axis,\n",
    "    seamnumbers,\n",
    "    mask_dataset,\n",
    "    relabel,\n",
    "    maxlabel,\n",
    "    in_place,\n",
    "    outputstem,\n",
    "    save_steps,\n",
    "]\n",
    "\n",
    "\n",
    "def get_arglist(args, axis, starts, stops, steps):\n",
    "    \"\"\"Replace the `axis` and `seamnumbers` arguments\n",
    "    with values specific for sequential zip-steps.\n",
    "    \n",
    "    axis = 0: zip-quads\n",
    "    axis = 1: zip-lines over Y\n",
    "    axis = 2: zip-lines over X\n",
    "    seamnumbers: start-stop-step triplets (with step=2)\n",
    "    \"\"\"\n",
    "\n",
    "    arglist = []\n",
    "    for seam_y in range(starts[0], stops[0], steps[0]):\n",
    "        for seam_x in range(starts[1], stops[1], steps[1]):\n",
    "            seamnumbers = [-1, seam_y, seam_x]\n",
    "            args[3] = axis\n",
    "            if axis == 0:\n",
    "                args[4] = [seamnumbers[d] if d != axis else -1 for d in [0, 1, 2]]\n",
    "            else:\n",
    "                args[4] = [seam_y if d == axis else -1 for d in [0, 1, 2]]\n",
    "            arglist.append(tuple(args))\n",
    "\n",
    "    return arglist\n",
    "\n",
    "\n",
    "def compute_zip_step(args, axis, seamgrid, starts, stops, steps, n_proc):\n",
    "    \"\"\"Compute the zip-step.\"\"\"\n",
    "\n",
    "    arglist = get_arglist(args, axis, starts, stops, steps)\n",
    "    print('submitting {:3d} jobs over {:3d} processes'.format(len(arglist), n_proc))\n",
    "\n",
    "    with multiprocessing.Pool(processes=n_proc) as pool:\n",
    "        pool.starmap(resegment_block_boundaries, arglist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convenience function that merges datablocks into a single volume and returns a single z-plane for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.mergeblocks import mergeblocks\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def merge_and_slice_dset(filelist, ids, dims, bs, bm, slc=20):\n",
    "\n",
    "    # Merge the datablocks.\n",
    "    images_in=['{}/{}'.format(datafile, ids)\n",
    "               for datafile in filelist]\n",
    "    filename = '{}.h5/{}'.format(dataset, ids)\n",
    "    outputpath=os.path.join(datadir, filename)\n",
    "\n",
    "    mergeblocks(\n",
    "        images_in=images_in,\n",
    "        outputpath=outputpath,\n",
    "        blocksize=[dims[0], bs, bs],\n",
    "        blockmargin=[0, bm, bm],\n",
    "        fullsize=dims[:3],\n",
    "    )\n",
    "\n",
    "    # Get a slice of the merged data.\n",
    "    im = Image(outputpath)\n",
    "    im.load()\n",
    "    im.slices[0] = slice(slc, slc + 1, 1)\n",
    "    data = im.slice_dataset()\n",
    "    im.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with the membrane mean blocks. This should output an image of 1408 x 1408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 'memb/mean'\n",
    "img = merge_and_slice_dset(filelist, ids, dims, bs, bm)\n",
    "\n",
    "plt.imshow(img, cmap='gray', vmax=5000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the zipping procedure, we employ an order such that no blocks are handled concurrently. First, blocks with overlap in the Y-dimension are processed (odd and even zip-lines separately); then X-ziplines; then the corners where four datablocks overlap are resegmented. For demo purpose, we keep track of the output for each step and store it in `imgs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 'segm/labels_memb_del_relabeled_fix'\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for axis, n_seams in zip([1, 2], n_seams_yx):\n",
    "\n",
    "    n_proc = min(n_proc_max, int(np.ceil(n_seams / 2)))\n",
    "\n",
    "    for offset in [0, 1]:\n",
    "\n",
    "        # do the zip-step\n",
    "        compute_zip_step(\n",
    "            args, axis, seamgrid,\n",
    "            starts=[offset, 0], stops=[n_seams, 1], steps=[2, 2],\n",
    "            n_proc=n_proc,\n",
    "        )\n",
    "\n",
    "        # update maxlabels\n",
    "        maxlabels = get_maxlabels_from_attribute(filelist, ids, maxlabelfile)\n",
    "\n",
    "        # keep image for display\n",
    "        imgs.append(merge_and_slice_dset(filelist, ids, dims, bs, bm))\n",
    "\n",
    "f, axs = plt.subplots(1, 4, figsize=(24, 24))\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newly processed zip-lines are assigned high labels indicated in yellow of the viridis colormap, nicely demonstrating the zipping process.\n",
    "\n",
    "The zip-lines still have seams in the places where they intersect. Next we process zip-quads, in which the segments on these intersections are resegmented to finish the zip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resegment zip-quads in 4 groups even/even, even/odd, odd/even, odd/odd zip-line intersections\n",
    "ids = 'segm/labels_memb_del_relabeled_fix'\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for start_y in [0, 1]:\n",
    "\n",
    "    for start_x in [0, 1]:\n",
    "\n",
    "        # do the zip-step\n",
    "        compute_zip_step(\n",
    "            args, axis=0, seamgrid=seamgrid,\n",
    "            starts=[start_y, start_x], stops=n_seams_yx, steps=[2, 2],\n",
    "            n_proc=n_proc,\n",
    "        )\n",
    "\n",
    "        # update maxlabels\n",
    "        maxlabels = get_maxlabels_from_attribute(filelist, ids, maxlabelfile)\n",
    "\n",
    "        # keep image for display\n",
    "        imgs.append(merge_and_slice_dset(filelist, ids, dims, bs, bm))\n",
    "\n",
    "f, axs = plt.subplots(1, 4, figsize=(24, 24))\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the segments in the more common random colors, we relabel, shuffle and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import relabel_sequential\n",
    "from skimage.color import label2rgb\n",
    "from random import shuffle\n",
    "\n",
    "img = merge_and_slice_dset(filelist, ids, dims, bs, bm)\n",
    "\n",
    "img = relabel_sequential(img)[0]\n",
    "\n",
    "ulabels = np.unique(img[:])[1:]\n",
    "relabeled = [l for l in range(0, len(ulabels))]\n",
    "shuffle(relabeled)\n",
    "\n",
    "img = np.array([0] + relabeled)[img]\n",
    "\n",
    "f = plt.figure(figsize=(12, 12))\n",
    "plt.imshow(label2rgb(img))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAPL-3D, we use rich multidimensional data to obtain a robust segmentation. We can also use the information we have to perform subcellular segmentation. Here, we split segments in nucleus and membrane subsegments such that we can specifically extract intensities from the appropriate voxels for the type of staining (nuclear or membranal). In addition, the subsegmentation opens up possibilities for defining compound features that inform on internal cell structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 'segm/labels_memb_del_relabeled_fix'\n",
    "seg_path = os.path.join(datadir, '{}.h5/{}'.format(dataset, ids))\n",
    "\n",
    "for ids1 in ['nucl/dapi_mask_sauvola', 'nucl/dapi_mask_absmin']:\n",
    "    merge_and_slice_dset(filelist, ids1, dims, bs, bm)\n",
    "\n",
    "from stapl3d.segmentation.segment import split_segments\n",
    "split_segments(seg_path, outputstem=filestem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a corner of the section to visualize the subcellular compartments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import LabelImage\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "slc = 20\n",
    "\n",
    "# get background images\n",
    "ids_n = 'nucl/dapi_preprocess'\n",
    "dapi = merge_and_slice_dset(filelist, ids_n, dims, bs, bm)\n",
    "ids_m = 'memb/mean_smooth'\n",
    "memb = merge_and_slice_dset(filelist, ids_m, dims, bs, bm)\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(24, 24))\n",
    "segs = [\n",
    "    'segm/labels_memb_del_relabeled_fix', \n",
    "    'segm/labels_memb_del_relabeled_fix_memb',\n",
    "    'segm/labels_memb_del_relabeled_fix_nucl',\n",
    "]\n",
    "bgs = [memb, dapi, memb]\n",
    "\n",
    "for ax, seg, bg in zip(axs, segs, bgs):\n",
    "    seg_path = os.path.join(datadir, '{}.h5/{}'.format(dataset, seg))\n",
    "    im = LabelImage(seg_path)\n",
    "    im.load()\n",
    "    im.slices[0] = slice(slc, slc + 1, 1)\n",
    "    img = im.slice_dataset()\n",
    "    im.close()\n",
    "\n",
    "    img = img[:500,:500]\n",
    "    bg = bg[:500,:500] * 5\n",
    "    clabels = label2rgb(img, image=bg, alpha=1.0, bg_label=0)\n",
    "    ax.imshow(clabels)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stapl3d",
   "language": "python",
   "name": "stapl3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
