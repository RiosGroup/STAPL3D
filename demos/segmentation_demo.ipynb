{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAPL-3D segmentation demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates the core components of the STAPL-3D segmentation pipeline: **blockwise segmentation** and **zipping**.\n",
    "\n",
    "If you did not follow the STAPL-3D README: please find STAPL-3D and the installation instructions [here](https://github.com/RiosGroup/STAPL3D) before doing this demo.\n",
    "\n",
    "Because STAPL-3D is all about big datafiles, we provide small cutouts and precomputed summary data that will be downloaded while progressing through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define where you have want the data to be downloaded by changing *projectdir*; default is the current demo directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import urllib.request\n",
    "\n",
    "projectdir = '.'\n",
    "dataset = 'HFK16w'\n",
    "\n",
    "datadir = os.path.join(projectdir, dataset)\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "filestem = os.path.join(datadir, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation import segment\n",
    "import yaml\n",
    "\n",
    "parameter_file = '{}.yml'.format(filestem)\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "cfg['segmentation']\n",
    "\n",
    "cfg['segmentation']['compactness'] = 0.002\n",
    "cfg['segmentation']['steps'] = [6, 7, 8]\n",
    "\n",
    "filepath = '/Users/michielkleinnijenhuis/projects/STAPL3D/demos/HFK16w/blocks/HFK16w_shading_stitching_biasfield_00000-00240_00000-00240_00000-00106.h5'\n",
    "segment.cell_segmentation(filepath, **cfg['segmentation'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define STAPL3D parameters preferably using a [yaml](https://yaml.org) parameter file. It has a simple structure and can be parsed in Python and `bash`. We will download the example, read it into a dictionary structure, list all entries and show the entry that contains information on the default directory structure for STAPL3D. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_file = '{}.yml'.format(filestem)\n",
    "\n",
    "# Download the yml-file.\n",
    "if not os.path.exists(parameter_file):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/Ubx9wVon5CIuIzo/download'\n",
    "    urllib.request.urlretrieve(url, parameter_file)\n",
    "\n",
    "# Load parameter file.\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "# Show contents.\n",
    "cfg.keys()\n",
    "cfg['dirtree']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provided a preprocessed data cutout in the Imaris v5.5 file format. which is an hdf5 file with 5 dimensions (free [Imaris Viewer](https://imaris.oxinst.com/imaris-viewer) is nowadays available; and the file format can be inspected with [HDFview](https://www.hdfgroup.org/downloads/hdfview/) or with `h5ls` or `h5py`.\n",
    "\n",
    "We download the file and name it according to the default STAPL-3D pipeline conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the ims-file.\n",
    "\n",
    "biasfield_stem = '{}{}{}{}'.format(\n",
    "    dataset,\n",
    "    cfg['shading']['postfix'],\n",
    "    cfg['stitching']['postfix'],\n",
    "    cfg['biasfield']['postfix'],\n",
    ")\n",
    "ims_filepath = os.path.join(datadir, '{}.ims'.format(biasfield_stem))\n",
    "\n",
    "if not os.path.exists(ims_filepath):\n",
    "    url = 'https://surfdrive.surf.nl/files/index.php/s/NxWhUWuLQBHPMGV/download'\n",
    "    urllib.request.urlretrieve(url, ims_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the STAPL-3D Image class to load this file and inspect it's properties. We'll also save the dimensions, the Z-dimension and the number of channels in convenience variables `dims`, `Z` and `C`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import Image\n",
    "\n",
    "image_in = ims_filepath\n",
    "im = Image(image_in)\n",
    "im.load(load_data=False)\n",
    "props = im.get_props()\n",
    "im.close()\n",
    "\n",
    "dims = im.dims\n",
    "Z = im.dims[im.axlab.index('z')]\n",
    "C = im.dims[im.axlab.index('c')]\n",
    "\n",
    "props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In processing the full dataset, this cutout of **106 x 1408 x 1408 x 8** would equate to a single datablock, but for this demo we will further subdivide this block to demonstrate the pipeline.\n",
    "\n",
    "For segmentation, we use a weighted sum of the membrane channels (ch3, ch5, ch6, ch7). The weights [0.5, 0.5, 1.0, 1.0] work well for this data.\n",
    "We have specified this in the parameter file HFK16w.yml:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "parameter_file = '{}.yml'.format(filestem)\n",
    "with open(parameter_file, 'r') as ymlfile:\n",
    "    cfg = yaml.safe_load(ymlfile)\n",
    "\n",
    "cfg['blocks']\n",
    "cfg['dataset']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above indicates that, in addition to the membrane sum, we generate a nuclear channel mean as well as a mean over all channels (used for generating masks). Importantly, we specify that we want to output channel 0 (DAPI), because we will use it to create a nuclear mask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the shape of the processing blocks. Usually we would opt for a blocksize of ~100-200 million voxels; now we chose a blocksize in *xy* of 176 for 64 blocks of ~6M voxels. We keep the margin similar to what we set for big datasets as reducing it may hinder adequate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = cfg['dataset']['bs']  # blocksize\n",
    "bm = cfg['dataset']['bm']  # blockmargin\n",
    "\n",
    "# NOTE: imaris layout: zyxct\n",
    "blocksize = [Z, bs, bs, C, 1]\n",
    "blockmargin = [0, bm, bm, 0, 0]\n",
    "\n",
    "blockdir = os.path.join(datadir, cfg['dirtree']['datadir']['blocks'])\n",
    "block_prefix = os.path.join(blockdir, biasfield_stem)\n",
    "os.makedirs(blockdir, exist_ok=True)\n",
    "\n",
    "'Processing data in blocks of {} voxels with a margin of {} voxels'.format(blocksize, blockmargin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the function that computes the membrane mean, and splits the data into blocks at the same time. Datablocks are written to the *HFK16w/blocks/* directory and are postfixed by the voxel coordinates of the original datafile HFK16w/blocks/HFK16w_**x-X_y_Y_z-Z**.h5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import blocks\n",
    "\n",
    "blocks.split(image_in, parameter_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some of the files that were generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "filelist = glob(os.path.join(blockdir, '{}_*.h5'.format(biasfield_stem)))\n",
    "filelist.sort()\n",
    "len(filelist), filelist[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting hdf5 files have the following internal file structure:\n",
    "    - .h5/mean\n",
    "    - .h5/chan/ch00\n",
    "    - .h5/memb/mean\n",
    "    - .h5/nucl/mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Membrane enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before segmentation, we perform membrane enhancement.\n",
    "\n",
    "For the demo we do not want to be dependent on the third-party [ACME](https://wiki.med.harvard.edu/SysBio/Megason/ACME) software and provide the output that otherwise results from the ACME procedure. We split it into blocks, and write it as separate datasets in the same files as the channel data.\n",
    "\n",
    "If you have ACME installed, set an `ACME` path environment variable or point `ACMEdir` to the directory with the binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACMEdir = os.environ.get('ACME')\n",
    "\n",
    "if ACMEdir:\n",
    "    # Perform membrane enhancement.\n",
    "    from stapl3d.segmentation import enhance\n",
    "    enhance.estimate(image_in, parameter_file)\n",
    "else:\n",
    "    # Download precomputed membrane enhancement.\n",
    "    acme_filepath = os.path.join(datadir, '{}_ACME.h5'.format(biasfield_stem))\n",
    "    if not os.path.exists(acme_filepath):\n",
    "        url = 'https://surfdrive.surf.nl/files/index.php/s/oQcxIocFBkaXwJe/download'\n",
    "        urllib.request.urlretrieve(url, acme_filepath)\n",
    "\n",
    "    # Split into blocks\n",
    "    from stapl3d import blocks\n",
    "    for ids in ['memb/preprocess', 'memb/planarity']:\n",
    "        acme_image = '{}/{}'.format(acme_filepath, ids)\n",
    "        output_template = '{}_{}.h5/{}'.format(block_prefix, '{}', ids)\n",
    "        blocks.splitblocks(acme_image, blocksize[:3], blockmargin[:3], output_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The segmentation is parallelized over the blocks we just created. Each of the 64 files is processed seperately.\n",
    "The segmentation routine is associated with a fair amount of parameters. This list all the parameters specified in the yml-file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(filelist), filelist[:5]\n",
    "cfg['segmentation']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few parameter of particular note:\n",
    "- input volumes:\n",
    "        'ids_memb_mask': 'memb/planarity'\n",
    "        'ids_memb_chan': 'memb/mean'\n",
    "        'ids_nucl_chan': 'chan/ch00'\n",
    "        'ids_dset_mean': 'mean'\n",
    "\n",
    "The following parameters can be changed to optimize segmentation or use parameters from automated fine tuning:\n",
    "- membrane mask:\n",
    "    - 'planarity_thr': 0.0005\n",
    "- nuclei mask:\n",
    "    - 'sauvola_window_size': [19, 75, 75]\n",
    "    - 'dapi_thr': 5000\n",
    "    - 'dapi_absmin': 1000\n",
    "- peak detection:\n",
    "    - 'peaks_size': [11, 19, 19]\n",
    "    - 'compactness': 0.8\n",
    "- watershed:\n",
    "    - 'memb_sigma': 3.0\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the segments for each block. Segmentation time of single block is in the minutes-range. The 106 x 240 x 240 blocksize (including the margin) will take ~1GB of memory per process. Please set the number of processes so that you will stay within RAM. `n_proc = 8` would be a fairly safe bet for modern systems; `n_proc = 0` results in using all available processors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_proc = 0\n",
    "\n",
    "from stapl3d.segmentation import segment\n",
    "segment.estimate(image_in, parameter_file, n_workers=n_proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report pages (pdf) have been written to the *HFK16w/blocks/* directory. Let's look at one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation.segment import generate_report\n",
    "\n",
    "block_idx = 20\n",
    "generate_report('{}/memb/mean'.format(filelist[block_idx]), ioff=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From left to right, images are show for:\n",
    " - the DAPI channel and the membrane mean\n",
    " - the nuclear mask and the membrane mask\n",
    " - the combined mask with detected peaks and overlaid on the distance transform image\n",
    " - the first and the final watershed results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having parallelized the segmentation process for increased analysis speed and reduced memory footprint, the need arises to reassemble the blocks into a final combined segmentation volume without seams at the block boundaries. These seams are a consequence of trivial parallelization in processing the individual blocks (i.e. without communication between the processes). They manifest through partial cells lying on the block boundaries that have been assigned different labels in different blocks. Importantly, these doubly segmented cells may not perfectly match up over the boundary. These block-boundary-segments need to be resegmented in order to complete the accurate segmentation of the full dataset. We refer to this correct reassembly of the datablocks as ‘zipping’. In short, it consists of identifying the segments lying on the boundaries, removing them, and resegmenting that space. We aimed to design the procedure such that it requires minimal computational resources and expertise (fast, with a low memory footprint, and without the need for communication between processes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relabel and copy blocks\n",
    "We first perform a sequential relabeling of all the blocks to make each label unique.\n",
    "We copy the relabeled blocks to new datasets in the same file for writing the zip-results in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d.segmentation import zipping\n",
    "\n",
    "# Relabel the blocks sequentially\n",
    "zipping.relabel(image_in, parameter_file)\n",
    "\n",
    "# Copy the relabeled segmentation\n",
    "zipping.copyblocks(image_in, parameter_file)\n",
    "\n",
    "# Write a file with the maximum of all the labels in the block.\n",
    "seg_pf = cfg['segmentation']['segments_ods']\n",
    "zip_pf = '{}_{}'.format(cfg['relabel']['postfix'], cfg['copyblocks']['postfix'])\n",
    "zip_path_int = 'segm/{}_{}'.format(seg_pf, zip_pf)\n",
    "\n",
    "from stapl3d import get_paths\n",
    "filestem = os.path.splitext(get_paths(image_in)['fname'])[0]\n",
    "filename = '{}_maxlabels_{}_{}.txt'.format(filestem, seg_pf, zip_pf)\n",
    "maxlabelfile = os.path.join(blockdir, filename)\n",
    "maxlabels = zipping.get_maxlabels_from_attribute(filelist, zip_path_int, maxlabelfile)\n",
    "'maxlabs after copy {}'.format(maxlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## zip\n",
    "There is a one-liner for computing all the steps in the zip:\n",
    "```\n",
    "zipping.estimate(image_in, parameter_file)\n",
    "```\n",
    "For this demo, we will be much more verbose to illustrate the zipping process. We use Python's multiprocessing for distributed processing if calling the functionality from within a Python interpreter. Next, we define the zipping parameters and functions. First, we set the number of processors, the block-layout and the zipping parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# Set the maximum number of processors to use\n",
    "n_proc_max = 8\n",
    "\n",
    "# Determine the number of seams in the data.\n",
    "n_seams_yx, seamgrid = zipping.get_zip_layout(image_in, blocksize)\n",
    "'Zipjob: {} seamlines over X; {} seamlines over Y'.format(n_seams_yx[1], n_seams_yx[0])\n",
    "\n",
    "# Argument list to `zipping.resegment_block_boundaries`\n",
    "images_in = ['{}/{}'.format(datafile, zip_path_int) for datafile in filelist]\n",
    "blocksize = blocksize[:3]\n",
    "blockmargin = blockmargin[:3]\n",
    "axis = 0\n",
    "seamnumbers = [-1, -1, -1]\n",
    "mask_dataset = ''\n",
    "relabel = False\n",
    "maxlabel = maxlabelfile\n",
    "in_place = True\n",
    "outputstem = os.path.join(blockdir, dataset)\n",
    "save_steps = False\n",
    "args = [images_in, blocksize, blockmargin, axis, seamnumbers,\n",
    "        mask_dataset, relabel, maxlabel, in_place, outputstem, save_steps]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a convenience function that merges datablocks into a single volume and returns a single z-plane for display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import blocks\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def merge_and_slice_dset(filelist, ids, dims, bs, bm, slc=20):\n",
    "\n",
    "    # Merge the datablocks.\n",
    "    images_in=['{}/{}'.format(datafile, ids)\n",
    "               for datafile in filelist]\n",
    "    filename = '{}.h5/{}'.format(dataset, ids)\n",
    "    outputpath=os.path.join(datadir, filename)\n",
    "\n",
    "    blocks.mergeblocks(\n",
    "        images_in=images_in,\n",
    "        blocksize=[dims[0], bs, bs],\n",
    "        blockmargin=[0, bm, bm],\n",
    "        fullsize=dims[:3],\n",
    "        outputpath=outputpath,\n",
    "    )\n",
    "\n",
    "    # Get a slice of the merged data.\n",
    "    im = Image(outputpath)\n",
    "    im.load()\n",
    "    im.slices[0] = slice(slc, slc + 1, 1)\n",
    "    data = im.slice_dataset()\n",
    "    im.close()\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check with the membrane mean blocks. This should output an image of 1408 x 1408."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = 'memb/mean'\n",
    "img = merge_and_slice_dset(filelist, ids, dims, bs, bm)\n",
    "\n",
    "plt.imshow(img, cmap='gray', vmax=5000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the zipping procedure, we employ an order such that no blocks are handled concurrently. First, blocks with overlap in the Y-dimension are processed (odd and even zip-lines separately); then X-ziplines; then the corners where four datablocks overlap are resegmented. For demo purpose, we keep track of the output for each step and store it in `imgs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resegment zip-lines in 4 groups: horizontal/even, horizontal/odd, vertical/even, vertical/odd zip-lines.\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for axis, n_seams in zip([1, 2], n_seams_yx):\n",
    "\n",
    "    n_proc = min(n_proc_max, int(np.ceil(n_seams / 2)))\n",
    "\n",
    "    for offset in [0, 1]:\n",
    "\n",
    "        # do the zip-step\n",
    "        zipping.compute_zip_step(\n",
    "            args, axis, seamgrid,\n",
    "            starts=[offset, 0], stops=[n_seams, 1], steps=[2, 2],\n",
    "            n_proc=n_proc,\n",
    "        )\n",
    "\n",
    "        # update maxlabels\n",
    "        maxlabels = zipping.get_maxlabels_from_attribute(filelist, zip_path_int, maxlabelfile)\n",
    "\n",
    "        # keep image for display\n",
    "        imgs.append(merge_and_slice_dset(filelist, zip_path_int, dims, bs, bm))\n",
    "\n",
    "f, axs = plt.subplots(1, 4, figsize=(24, 24))\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newly processed zip-lines are assigned high labels indicated in yellow of the viridis colormap, nicely demonstrating the zipping process.\n",
    "\n",
    "The zip-lines still have seams in the places where they intersect. Next we process zip-quads, in which the segments on these intersections are resegmented to finish the zip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resegment zip-quads in 4 groups: even/even, even/odd, odd/even, odd/odd zip-line intersections\n",
    "\n",
    "imgs = []\n",
    "\n",
    "for start_y in [0, 1]:\n",
    "\n",
    "    for start_x in [0, 1]:\n",
    "\n",
    "        # do the zip-step\n",
    "        zipping.compute_zip_step(\n",
    "            args, axis=0, seamgrid=seamgrid,\n",
    "            starts=[start_y, start_x], stops=n_seams_yx, steps=[2, 2],\n",
    "            n_proc=n_proc,\n",
    "        )\n",
    "\n",
    "        # update maxlabels\n",
    "        maxlabels = zipping.get_maxlabels_from_attribute(filelist, zip_path_int, maxlabelfile)\n",
    "\n",
    "        # keep image for display\n",
    "        imgs.append(merge_and_slice_dset(filelist, zip_path_int, dims, bs, bm))\n",
    "\n",
    "f, axs = plt.subplots(1, 4, figsize=(24, 24))\n",
    "for img, ax in zip(imgs, axs):\n",
    "    ax.imshow(img)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the segments in the more common random colors, we relabel, shuffle and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.segmentation import relabel_sequential\n",
    "from skimage.color import label2rgb\n",
    "from random import shuffle\n",
    "\n",
    "img = merge_and_slice_dset(filelist, zip_path_int, dims, bs, bm)\n",
    "\n",
    "img = relabel_sequential(img)[0]\n",
    "\n",
    "ulabels = np.unique(img[:])[1:]\n",
    "relabeled = [l for l in range(0, len(ulabels))]\n",
    "shuffle(relabeled)\n",
    "\n",
    "img = np.array([0] + relabeled)[img]\n",
    "\n",
    "f = plt.figure(figsize=(12, 12))\n",
    "plt.imshow(label2rgb(img))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In STAPL-3D, we use rich multidimensional data to obtain a robust segmentation. We can also use the information we have to perform subcellular segmentation. Here, we split segments in nucleus and membrane subsegments such that we can specifically extract intensities from the appropriate voxels for the type of staining (nuclear or membranal). In addition, the subsegmentation opens up possibilities for defining compound features that inform on internal cell structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = os.path.join(datadir, '{}.h5/{}'.format(dataset, zip_path_int))\n",
    "\n",
    "from stapl3d.segmentation import segment\n",
    "segment.subsegment(image_in, parameter_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pf in ['full', 'memb', 'nucl']:\n",
    "    ids = 'segm/labels_memb_del_relabeled_fix_{}'.format(pf)\n",
    "    merge_and_slice_dset(filelist, zip_path_int, dims, bs, bm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at a corner of the section to visualize the subcellular compartments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stapl3d import LabelImage\n",
    "from skimage.color import label2rgb\n",
    "\n",
    "slc = 20\n",
    "\n",
    "# get background images\n",
    "ids_n = 'nucl/dapi_preprocess'\n",
    "dapi = merge_and_slice_dset(filelist, ids_n, dims, bs, bm)\n",
    "ids_m = 'memb/mean_smooth'\n",
    "memb = merge_and_slice_dset(filelist, ids_m, dims, bs, bm)\n",
    "\n",
    "f, axs = plt.subplots(1, 3, figsize=(24, 24))\n",
    "segs = [\n",
    "    'segm/labels_memb_del_relabeled_fix_full', \n",
    "    'segm/labels_memb_del_relabeled_fix_memb',\n",
    "    'segm/labels_memb_del_relabeled_fix_nucl',\n",
    "]\n",
    "bgs = [memb, dapi, memb]\n",
    "\n",
    "for ax, seg, bg in zip(axs, segs, bgs):\n",
    "    seg_path = os.path.join(datadir, '{}.h5/{}'.format(dataset, seg))\n",
    "    im = LabelImage(seg_path)\n",
    "    im.load()\n",
    "    im.slices[0] = slice(slc, slc + 1, 1)\n",
    "    img = im.slice_dataset()\n",
    "    im.close()\n",
    "\n",
    "    img = img[:500,:500]\n",
    "    bg = bg[:500,:500] * 5\n",
    "    clabels = label2rgb(img, image=bg, alpha=1.0, bg_label=0)\n",
    "    ax.imshow(clabels)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stapl3d",
   "language": "python",
   "name": "stapl3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
